{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET TWEETS DATA WITH API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA CLEANING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "stream = open('',encoding='utf-8')\n",
    "message = stream.read()\n",
    "\n",
    "print(type(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows-1254\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "with open('DATA TEXT/CHPfatihportakal.txt', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "print(result['encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               'ü•∫'\n",
    "                               'ü§¶'\n",
    "                               'ü§Æ'\n",
    "                               'ü§©'\n",
    "                               'ü§õ'\n",
    "                               'ü§≤'\n",
    "                               'ü§¨'\n",
    "                               'ü§î'\n",
    "                               'ü§£'\n",
    "                               'ü§∑‚Äç'\n",
    "                               'ü§™'\n",
    "                               'ü•∞'\n",
    "                               'ü§Ø'\n",
    "                               'ü•á'\n",
    "                               'ü•â'\n",
    "                               'ü•Ä'\n",
    "                               'ü§≠'\n",
    "                               'ü•≥'\n",
    "                               'üßê'\n",
    "                               'ü§ò'\n",
    "                               'ü§ö'\n",
    "                               'ü§ì'\n",
    "                               '‚è∞'\n",
    "                               'ü•≤'\n",
    "                               'ü§ó'\n",
    "                               'üßë'\n",
    "                               'ü¶≥'\n",
    "                               '‚ù§Ô∏è'\n",
    "                               'üßø'\n",
    "                               'ü§ï'\n",
    "                               'ü§ù'\n",
    "                               ': ) '\n",
    "                               '( '\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "f_path = 'DATA TEXT/'\n",
    "\n",
    "for i in os.listdir(f_path):\n",
    "\n",
    "\n",
    "    with open(f'DATA TEXT/{i}', 'r',encoding='utf-8') as f:\n",
    "        metin = f.read()\n",
    "\n",
    "\n",
    "    # Satƒ±r bo≈üluklarƒ±nƒ± sil\n",
    "    metin = re.sub(r'http\\S+', '', metin)  #remove all links\n",
    "    metin=remove_emoji(metin)#remove all emojis \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Yeni metin i√ßeriƒüini dosyaya yaz\n",
    "    with open(f'new/{i}.txt', 'w',encoding='utf-8') as f:\n",
    "        f.write(metin )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas \n",
    " \n",
    "\n",
    "AKP=0\n",
    "CHP=1\n",
    "IYIP=2\n",
    "HDP=3\n",
    "\n",
    "\n",
    "def txt_to_csv(file_name,party):\n",
    "    # TXT dosyasƒ±nƒ± DataFrame olarak oku\n",
    "\n",
    "    df = pd.read_table(file_name, header=None, names=['tweet'])\n",
    "    df['Parti'] = party\n",
    "\n",
    "    csv_path = file_name.replace('.txt', '.csv')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='new/'\n",
    "for i in os.listdir(file_name):\n",
    "    if i.startswith(\"CHP\"):\n",
    "        txt_to_csv(f'new/{i}',1)\n",
    "    if i.startswith(\"AKP\"):\n",
    "        txt_to_csv(f'new/{i}',0)\n",
    "    if i.startswith(\"HDP\"):\n",
    "        txt_to_csv(f'new/{i}',3)\n",
    "    if i.startswith(\"IYIP\"):\n",
    "        txt_to_csv(f'new/{i}',2)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bunu yapma sebebimiz turkce de zemberek kutuphanesini normallestirme ve yazim yanlislarini duzeltecegimiz zaman sorun cikmamasi icin.\n",
    "df=pd.read_csv('new/AKPCumhurFrankfurt.csv')\n",
    "import pandas as pd\n",
    "\n",
    "# √∂rnek bir DataFrame olu≈üturalƒ±m\n",
    "noktalama_isaretleri = ['.', '!', '?']\n",
    "for i in range(len(df)):\n",
    "    if df.loc[i, 'tweet'][-1] not in noktalama_isaretleri:\n",
    "        df.loc[i, 'tweet'] += '.'  # sonuna nokta ekle\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "noktalama_isaretleri = ['.', '!', '?','\"']  # kontrol edilecek noktalama i≈üaretleri\n",
    "\n",
    "folder_path = 'new/'  # klas√∂r yolunu belirleyin\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):  # sadece CSV dosyalarƒ± i√ßin i≈ülem yap\n",
    "        file_path = os.path.join(folder_path, filename)  # dosya yolunu olu≈ütur\n",
    "        df = pd.read_csv(file_path)  # CSV dosyasƒ±nƒ± oku\n",
    "        for i in range(len(df)):\n",
    "            if df.loc[i, 'tweet'][-1] not in noktalama_isaretleri:\n",
    "                df.loc[i, 'tweet'] += '.'  # sonuna nokta ekle\n",
    "        df.to_csv(file_path, index=False)  # CSV dosyasƒ±nƒ± yeniden yaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "import pyspark\n",
    "from zemberek import (\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "sc = pyspark.SparkConf()\n",
    "#\n",
    "spark=SparkSession.builder.appName('test').getOrCreate()\n",
    "from pyspark import SparkContext \n",
    "sc = SparkContext.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-09 16:05:16,055 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 7.074776887893677\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from zemberek import (\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")\n",
    "\n",
    "\n",
    "morphology = TurkishMorphology.create_with_defaults()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'new/'  \n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):  \n",
    "        file_path = os.path.join(folder_path, filename)  \n",
    "        df = pd.read_csv(file_path) \n",
    "        morphology = TurkishMorphology.create_with_defaults()\n",
    "        normalizer = TurkishSentenceNormalizer(morphology)\n",
    "        for example in df['tweet']:\n",
    "            print(example)\n",
    "            print(normalizer.normalize(example), \"\\n\")\n",
    "            df=normalizer.normalize(example), \"\\n\"\n",
    "            sc = TurkishSpellChecker(morphology)\n",
    "        df.to_csv(file_path, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = 'new/'  \n",
    "new_folder_path = 'normal/'\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):  \n",
    "        file_path = os.path.join(folder_path, filename)  \n",
    "        df_pyspark = spark.read.option('header', 'true').csv(f'{file_path}', inferSchema=True, header=True)\n",
    "\n",
    "        tweets = df_pyspark.select(col(\"tweet\"))\n",
    "\n",
    "        morphology = TurkishMorphology.create_with_defaults()\n",
    "        normalizer = TurkishSentenceNormalizer(morphology)\n",
    "\n",
    "        normalized_tweets = []\n",
    "        for row in tweets.rdd.collect():\n",
    "            tweet = row[0]  \n",
    "            print(tweet)\n",
    "            normalized_tweet = normalizer.normalize(tweet)\n",
    "            print(normalized_tweet, \"\\n\")\n",
    "            normalized_tweets.append(normalized_tweet)\n",
    "\n",
    "        # Create a new DataFrame from the normalized tweets list\n",
    "        df_normalized = pd.DataFrame({'normalized_tweet': normalized_tweets})\n",
    "\n",
    "        # Save the DataFrame to a CSV file in the 'normalized' folder\n",
    "        normalized_file_path = os.path.join(new_folder_path, f'{filename}')\n",
    "        df_normalized.to_csv(normalized_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Parti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nev≈üin Hocamƒ±z ile Sahura Doƒüru https://t.co/E...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bastƒ±ƒüƒ±n yerleri ‚Äútoprak‚Äù diyerek ge√ßme, tanƒ±‚Ä¶...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bir oy Erdoƒüan‚Äôa, bir oy Kƒ±zƒ±lelma‚Äôya.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bu se√ßim atmosferinde nasƒ±l olacak bilmiyorum ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Refah Partisi‚Äônin kararƒ±ndan sonra ya≈üadƒ±ƒüƒ±m b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852</th>\n",
       "      <td>Yaptƒ±ƒüƒ± √ßalƒ±≈ümalarla hem i√ßeride hem dƒ±≈üarida ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>Azerbaycan‚Äôƒ±n Ermenistan‚Äôa yakla≈üƒ±mƒ±;Bu olmadƒ±...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>Deƒüi≈üik ve √∂nemli bir sƒ±nƒ±flandƒ±rma ü§î https://...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>ƒ∞yi ki bu mekan K√ºlt√ºr bakanlƒ±ƒüƒ±nƒ±n elindeƒ∞yi ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1856</th>\n",
       "      <td>Bu net tavrƒ±ne √ßooook beklemi≈üiz, ne hastetini...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10213 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  Parti\n",
       "0     Nev≈üin Hocamƒ±z ile Sahura Doƒüru https://t.co/E...      0\n",
       "1     Bastƒ±ƒüƒ±n yerleri ‚Äútoprak‚Äù diyerek ge√ßme, tanƒ±‚Ä¶...      0\n",
       "2                Bir oy Erdoƒüan‚Äôa, bir oy Kƒ±zƒ±lelma‚Äôya.      0\n",
       "3     Bu se√ßim atmosferinde nasƒ±l olacak bilmiyorum ...      0\n",
       "4     Refah Partisi‚Äônin kararƒ±ndan sonra ya≈üadƒ±ƒüƒ±m b...      0\n",
       "...                                                 ...    ...\n",
       "1852  Yaptƒ±ƒüƒ± √ßalƒ±≈ümalarla hem i√ßeride hem dƒ±≈üarida ...      0\n",
       "1853  Azerbaycan‚Äôƒ±n Ermenistan‚Äôa yakla≈üƒ±mƒ±;Bu olmadƒ±...      0\n",
       "1854  Deƒüi≈üik ve √∂nemli bir sƒ±nƒ±flandƒ±rma ü§î https://...      0\n",
       "1855  ƒ∞yi ki bu mekan K√ºlt√ºr bakanlƒ±ƒüƒ±nƒ±n elindeƒ∞yi ...      0\n",
       "1856  Bu net tavrƒ±ne √ßooook beklemi≈üiz, ne hastetini...      0\n",
       "\n",
       "[10213 rows x 2 columns]"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "folder_path = 'DATA CSV/'\n",
    "df_chp = []\n",
    "df_akp=[]\n",
    "df_iyip=[]\n",
    "df_hdp=[]\n",
    "\n",
    "\n",
    "for i in os.listdir(folder_path):\n",
    "    if i.startswith(\"CHP\"):\n",
    "        file_path = os.path.join(folder_path, i)\n",
    "        dfchp = pd.read_csv(file_path)\n",
    "        df_chp.append(dfchp)\n",
    "    elif i.startswith(\"AKP\"):\n",
    "        file_path = os.path.join(folder_path, i)\n",
    "        dfakp = pd.read_csv(file_path)\n",
    "        df_akp.append(dfakp)    \n",
    "    elif i.startswith(\"IYIP\"):\n",
    "        file_path = os.path.join(folder_path, i)\n",
    "        dfiyip = pd.read_csv(file_path)\n",
    "        df_iyip.append(dfiyip)\n",
    "    else:\n",
    "        file_path = os.path.join(folder_path, i)\n",
    "        dfhdp = pd.read_csv(file_path)\n",
    "        df_hdp.append(dfhdp) \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "chpscombined_df = pd.concat(df_chp)\n",
    "akpscombined_df = pd.concat(df_akp)\n",
    "hdpscombined_df = pd.concat(df_hdp)\n",
    "iyipscombined_df = pd.concat(df_iyip)\n",
    "akpscombined_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Birlestirdigimiz verilerimizde null deger var mi diye kontrol etme zamani "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "akpscombined_df['tweet'].isnull().values.any()\n",
    "chpscombined_df['tweet'].isnull().values.any()\n",
    "iyipscombined_df['tweet'].isnull().values.any()\n",
    "hdpscombined_df['tweet'].isnull().values.any()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partilerden oy oranlarina kiyasla aldigimiz kadar veri adedini isliyoruz bunu yapma sebebimiz mikro olcekte karsimiza yine cikacak olan verilerin orani bu orana yakin olacak simdi elimizdeki verilerin adedini kontrol etme zamani "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AKPden toplamda 10213 tane tweetimiz var.\n",
      "CHPden toplamda 7049 tane tweetimiz var.\n",
      "HDPden toplamda 4248 tane tweetimiz var.\n",
      "IYIPden toplamda 6549 tane tweetimiz var.\n",
      "toplamda butun partilerde 28059 tane islenecek tweetimiz var.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Parti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ƒ∞yiliƒüin kar≈üƒ±sƒ±nda kur≈üunlarƒ±nƒ±zƒ±n bile etkis...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3 gen√ß karde≈üim oturmu≈ü beste yapmƒ±≈ülar, s√∂z y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kadƒ±n cinayetlerinde: ‚Äú√áok seviyordum o nedenl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>√ñn√ºm√ºzdeki se√ßim bu anlayƒ±≈üa kar≈üƒ± yapacaƒüƒ±mƒ±z...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ƒ∞ddialƒ± s√∂z s√∂ylemeyi sevmem ancak iddialƒ± i≈ül...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>Yarƒ±n... https://t.co/7s6KJiDCbs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>Mara≈ühttps://t.co/1upKGB8daH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>Bu meseleyi Can D√ºndar‚Äôƒ±n meselesi zanneden ya...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>ƒ∞ki g√ºn kaldƒ±... https://t.co/5TO6aTAOWi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>Entarili hekimhttps://t.co/fAG0ErFhqz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7049 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  Parti\n",
       "0     ƒ∞yiliƒüin kar≈üƒ±sƒ±nda kur≈üunlarƒ±nƒ±zƒ±n bile etkis...      1\n",
       "1     3 gen√ß karde≈üim oturmu≈ü beste yapmƒ±≈ülar, s√∂z y...      1\n",
       "2     Kadƒ±n cinayetlerinde: ‚Äú√áok seviyordum o nedenl...      1\n",
       "3     √ñn√ºm√ºzdeki se√ßim bu anlayƒ±≈üa kar≈üƒ± yapacaƒüƒ±mƒ±z...      1\n",
       "4     ƒ∞ddialƒ± s√∂z s√∂ylemeyi sevmem ancak iddialƒ± i≈ül...      1\n",
       "...                                                 ...    ...\n",
       "1647                   Yarƒ±n... https://t.co/7s6KJiDCbs      1\n",
       "1648                       Mara≈ühttps://t.co/1upKGB8daH      1\n",
       "1649  Bu meseleyi Can D√ºndar‚Äôƒ±n meselesi zanneden ya...      1\n",
       "1650           ƒ∞ki g√ºn kaldƒ±... https://t.co/5TO6aTAOWi      1\n",
       "1651              Entarili hekimhttps://t.co/fAG0ErFhqz      1\n",
       "\n",
       "[7049 rows x 2 columns]"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'AKPden toplamda {akpscombined_df.shape[0]} tane tweetimiz var.')\n",
    "print(f'CHPden toplamda {chpscombined_df.shape[0]} tane tweetimiz var.')\n",
    "print(f'HDPden toplamda {hdpscombined_df.shape[0]} tane tweetimiz var.')\n",
    "print(f'IYIPden toplamda {iyipscombined_df.shape[0]} tane tweetimiz var.')\n",
    "\n",
    "\n",
    "toplam_tweet_sayisi = pd.concat([akpscombined_df, chpscombined_df,hdpscombined_df,iyipscombined_df])\n",
    "toplam_tweet_sayisi.shape\n",
    "print(f'toplamda butun partilerde {toplam_tweet_sayisi.shape[0]} tane islenecek tweetimiz var.')\n",
    "akpscombined_df\n",
    "chpscombined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Parti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ƒ∞yiliƒüin kar≈üƒ±sƒ±nda kur≈üunlarƒ±nƒ±zƒ±n bile etkis...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3 gen√ß karde≈üim oturmu≈ü beste yapmƒ±≈ülar, s√∂z y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kadƒ±n cinayetlerinde: ‚Äú√áok seviyordum o nedenl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>√ñn√ºm√ºzdeki se√ßim bu anlayƒ±≈üa kar≈üƒ± yapacaƒüƒ±mƒ±z...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ƒ∞ddialƒ± s√∂z s√∂ylemeyi sevmem ancak iddialƒ± i≈ül...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7044</th>\n",
       "      <td>Yarƒ±n... marlborooooo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7045</th>\n",
       "      <td>Mara≈ümarlborooooo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7046</th>\n",
       "      <td>Bu meseleyi Can D√ºndar‚Äôƒ±n meselesi zanneden ya...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7047</th>\n",
       "      <td>ƒ∞ki g√ºn kaldƒ±... marlborooooo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7048</th>\n",
       "      <td>Entarili hekimmarlborooooo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7049 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  Parti\n",
       "0     ƒ∞yiliƒüin kar≈üƒ±sƒ±nda kur≈üunlarƒ±nƒ±zƒ±n bile etkis...      1\n",
       "1     3 gen√ß karde≈üim oturmu≈ü beste yapmƒ±≈ülar, s√∂z y...      1\n",
       "2     Kadƒ±n cinayetlerinde: ‚Äú√áok seviyordum o nedenl...      1\n",
       "3     √ñn√ºm√ºzdeki se√ßim bu anlayƒ±≈üa kar≈üƒ± yapacaƒüƒ±mƒ±z...      1\n",
       "4     ƒ∞ddialƒ± s√∂z s√∂ylemeyi sevmem ancak iddialƒ± i≈ül...      1\n",
       "...                                                 ...    ...\n",
       "7044                              Yarƒ±n... marlborooooo      1\n",
       "7045                                  Mara≈ümarlborooooo      1\n",
       "7046  Bu meseleyi Can D√ºndar‚Äôƒ±n meselesi zanneden ya...      1\n",
       "7047                      ƒ∞ki g√ºn kaldƒ±... marlborooooo      1\n",
       "7048                         Entarili hekimmarlborooooo      1\n",
       "\n",
       "[7049 rows x 2 columns]"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "akpscombined_df['tweet'] = akpscombined_df['tweet'].str.replace(r'http\\S+|www.\\S+', '', regex=True)\n",
    "chpscombined_df['tweet'] = chpscombined_df['tweet'].str.replace(r'http\\S+|www.\\S+', '', regex=True)\n",
    "hdpscombined_df['tweet'] = hdpscombined_df['tweet'].str.replace(r'http\\S+|www.\\S+', '', regex=True)\n",
    "\n",
    "iyipscombined_df['tweet'] = iyipscombined_df['tweet'].str.replace(r'http\\S+|www.\\S+', '', regex=True)\n",
    "\n",
    "akpscombined_df = akpscombined_df.reset_index(drop=True)\n",
    "chpscombined_df=chpscombined_df.reset_index(drop=True)\n",
    "hdpscombined_df=hdpscombined_df.reset_index(drop=True)\n",
    "iyipscombined_df=iyipscombined_df.reset_index(drop=True)\n",
    "chpscombined_df\n",
    "\n",
    "\n",
    "#Linklerin yerine marlbroo koyma sebebim bazi satirlar sadece bu sekilde basliyor ve onlari startswith komutu ile drop yapmak \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               'ü•∫'\n",
    "                               'ü§¶'\n",
    "                               'ü§Æ'\n",
    "                               'ü§©'\n",
    "                               'ü§õ'\n",
    "                               'ü§≤'\n",
    "                               'ü§¨'\n",
    "                               'ü§î'\n",
    "                               'ü§£'\n",
    "                               'ü§∑‚Äç'\n",
    "                               'ü§™'\n",
    "                               'ü•∞'\n",
    "                               'ü§Ø'\n",
    "                               'ü•á'\n",
    "                               'ü•â'\n",
    "                               'ü§≠'\n",
    "                               'ü•≥'\n",
    "                               'üßê'\n",
    "                               'ü§ò'\n",
    "                               'ü§ö'\n",
    "                               'ü§ì'\n",
    "                               '‚è∞'\n",
    "                               'ü•≤'\n",
    "                               'ü§ó'\n",
    "                               'üßë'\n",
    "                               'ü¶≥'\n",
    "                               '‚ù§Ô∏è'\n",
    "                               'üßø'\n",
    "                               'ü§ù'\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "akpscombined_df['tweet'] = akpscombined_df['tweet'].apply(remove_emoji)\n",
    "chpscombined_df['tweet'] =chpscombined_df['tweet'].apply(remove_emoji)\n",
    "hdpscombined_df['tweet'] =hdpscombined_df['tweet'].apply(remove_emoji)\n",
    "iyipscombined_df['tweet'] =iyipscombined_df['tweet'].apply(remove_emoji)\n",
    "hdpscombined_df.to_csv('hdp.csv',index=False)\n",
    "akpscombined_df.to_csv('akp.csv',index=False)\n",
    "chpscombined_df.to_csv('chp.csv',index=False)\n",
    "iyipscombined_df.to_csv('iyip.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "akpscombined_df=pandas.read_csv('akp.csv')\n",
    "chpscombined_df=pandas.read_csv('chp.csv')\n",
    "hdpscombined_df=pandas.read_csv('hdp.csv')\n",
    "iyipscombined_df=pandas.read_csv('iyip.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noktalamalari cikariyouz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noktalamalari cikariyouz\n",
    "chpscombined_df['tweet'] = chpscombined_df['tweet'].apply(lambda x: re.sub(r'[^\\w\\s]','',str(x)))\n",
    "hdpscombined_df['tweet']= hdpscombined_df['tweet'].apply(lambda x: re.sub(r'[^\\w\\s]','',str(x)))\n",
    "iyipscombined_df['tweet']= iyipscombined_df['tweet'].apply(lambda x: re.sub(r'[^\\w\\s]','',str(x)))\n",
    "akpscombined_df['tweet']= akpscombined_df['tweet'].apply(lambda x: re.sub(r'[^\\w\\s]','',str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Parti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7365</th>\n",
       "      <td>Vuuuuuuuuuuuuu Bam bam bam</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7366</th>\n",
       "      <td>Ota boka b√∂rt√ºye b√∂ceƒüe tweet atan √ºnl√º soytar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7367</th>\n",
       "      <td>Allah cc T√ºrk ordusunu muzaffer etsin G√ºn birl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7368</th>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  Parti\n",
       "7365                        Vuuuuuuuuuuuuu Bam bam bam       0\n",
       "7366  Ota boka b√∂rt√ºye b√∂ceƒüe tweet atan √ºnl√º soytar...      0\n",
       "7367  Allah cc T√ºrk ordusunu muzaffer etsin G√ºn birl...      0\n",
       "7368                                                nan      0"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "akpscombined_df.to_csv('nopuncakp.csv',index=False)\n",
    "hdpscombined_df.to_csv('nopunchdp.csv',index=False)\n",
    "iyipscombined_df.to_csv('nopunciyip.csv',index=False)\n",
    "chpscombined_df.to_csv('nopuncchp.csv',index=False)\n",
    "akpscombined_df[7365:7369]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deneme=pandas.DataFrame(akpscombined_df['tweet'])\n",
    "type(deneme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(deneme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[1072, 1723, 1976, 1977, 1982, 2032, 2033, 2041, 2075, 2174, 2195, 2200, 2217, 2272, 2337, 2358, 2386, 2420, 2443, 2483, 2513, 2547, 2551, 2556, 2560, 2574, 2604, 3607, 3736, 3745, 3854, 3867, 3872, 3888, 3889, 3904, 3917, 3918, 3924]\n"
     ]
    }
   ],
   "source": [
    "deet=pd.read_csv('nopunciyip.csv')\n",
    "\n",
    "type(deet)\n",
    "dooot= deet['tweet'].apply(lambda x: str(x).startswith('nan'))\n",
    "\n",
    "print(dooot.any())\n",
    "true_indices = dooot[dooot == True].index.tolist()\n",
    "\n",
    "print(true_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Kale:Noun, Prop] kale:Noun+A3sg+m:P1sg+in:Gen\n",
      "[kale:Noun] kale:Noun+A3sg+m:P1sg+in:Gen\n",
      "[kalem:Noun] kalem:Noun+A3sg+in:Gen\n",
      "[kalem:Noun] kalem:Noun+A3sg+in:P2sg\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SINGLE WORD MORPHOLOGICAL ANALYSIS\n",
    "results = morphology.analyze(\"kalemin\")\n",
    "for result in results:\n",
    "    print(result)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-11 10:27:42,046 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 32.35727047920227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from zemberek import (\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")\n",
    "\n",
    "m = TurkishMorphology.create_with_defaults()\n",
    "\n",
    "# SENTENCE ANALYSIS AND DISAMBIGUATION\n",
    "\n",
    "sentence = \"ƒ∞HA Tƒ∞HA Sƒ∞HA H√úRKU≈û TORAKS KAAN ATAK ANKA PEN√áE STAMP SARP HGK LGK KMC Hƒ∞SAR Solculara √∂rg√ºt ismi bƒ±rakmamƒ±≈üƒ±z Kudurmalarƒ± doƒüal\"\n",
    "analysis = m.analyze_sentence(sentence)\n",
    "after = m.disambiguate(sentence, analysis)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iha\n",
      " tiha\n",
      " Sƒ∞HA\n",
      " h√ºrku≈ü\n",
      " toraks\n",
      " kaan\n",
      " atak\n",
      " anka\n",
      " pen√ße\n",
      " stamp\n",
      " sarp\n",
      " hgk\n",
      " LGK\n",
      " KMC\n",
      " hisar\n",
      " sol\n",
      " √∂rg√ºt\n",
      " ism\n",
      " bƒ±rak\n",
      " kudur\n",
      " doƒüal\n"
     ]
    }
   ],
   "source": [
    "for s in after.best_analysis():\n",
    "    x = str(s)\n",
    "    word = x.split(\":\")[1].split(\"]\")[-1]\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' g√ºn'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TOGG\n",
      " g√ºn\n",
      " sene\n",
      " sipari≈ü\n",
      " al\n",
      " milli\n",
      " muharip\n",
      " u√ßaƒü\n",
      " bismillah\n",
      " de\n",
      " tarih\n",
      " en\n",
      " b√ºy√ºk\n",
      " afet\n",
      " konut\n",
      " seferber\n",
      " kaz\n",
      " vurul\n",
      " diƒüer\n",
      " bak\n",
      " diploma\n",
      " sor\n",
      " diƒüer\n",
      " cb\n",
      " yardƒ±mcƒ±\n",
      " g√∂rev\n",
      " tanƒ±m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content =  Saat\n",
      "Type =  Word\n",
      "Start =  0\n",
      "Stop =  3 \n",
      "\n",
      "Content =  kac\n",
      "Type =  Word\n",
      "Start =  5\n",
      "Stop =  7 \n",
      "\n",
      "Content =  ben\n",
      "Type =  Word\n",
      "Start =  9\n",
      "Stop =  11 \n",
      "\n",
      "Content =  nerden\n",
      "Type =  Word\n",
      "Start =  13\n",
      "Stop =  18 \n",
      "\n",
      "Content =  bileyim\n",
      "Type =  Word\n",
      "Start =  20\n",
      "Stop =  26 \n",
      "\n",
      "Content =  .\n",
      "Type =  Punctuation\n",
      "Start =  27\n",
      "Stop =  27 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TOKENIZATION\n",
    "tokenizer = TurkishTokenizer.DEFAULT\n",
    "\n",
    "tokens = tokenizer.tokenize(\"Saat kac ben nerden bileyim.\")\n",
    "for token in tokens:\n",
    "    print('Content = ', token.content)\n",
    "    print('Type = ', token.type_.name)\n",
    "    print('Start = ', token.start)\n",
    "    print('Stop = ', token.end, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "with open('islevsiz.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = [word.strip() for word in file.readlines()]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    clean_words = [word for word in words if word.lower() not in stopwords]\n",
    "    return ' '.join(clean_words)\n",
    "\n",
    "\n",
    "folder_path = 'soooo/'\n",
    "new_folder_path = 'nostopwords/'\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['tweet'] = df['tweet'].apply(remove_stopwords)\n",
    "        normalized_file_path = os.path.join(new_folder_path, f'{filename}')\n",
    "\n",
    "        df.to_csv(normalized_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Removes punctuation marks from a text string, except for \"#\" and \"@\" symbols.\n",
    "    \"\"\"\n",
    "    punctuation = string.punctuation.replace(\"#\", \"\").replace(\"@\", \"\")\n",
    "    translator = str.maketrans(\"\", \"\", punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "folder_path = ''\n",
    "new_folder_path = 'no punc/'\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['tweet'] = df['tweet'].apply(remove_punctuation)\n",
    "\n",
    "        normalized_file_path = os.path.join(new_folder_path, f'{filename}')\n",
    "\n",
    "        df.to_csv(normalized_file_path, index=False)\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'no punc/'\n",
    "new_folder_path = 'son/'\n",
    "\n",
    "\n",
    "import re\n",
    "def remove_digits(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['normalized_tweet'] = df['normalized_tweet'].str.replace('‚Äú', '').str.replace(\"‚Äù\", \"\").str.replace(\"‚Ä¶\",'').str.replace('‚Äô','').str.replace('‚Äò','')\n",
    "        #df['normalized_tweet'] = df['normalized_tweet'].apply(lambda x: x.strip())\n",
    "        df['normalized_tweet'] = df['normalized_tweet'].astype(str).apply(remove_digits)\n",
    "        df['normalized_tweet'] = df['normalized_tweet'].astype(str).apply(lambda x: x.strip())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        normalized_file_path = os.path.join(new_folder_path, f'{filename}')\n",
    "\n",
    "        df.to_csv(normalized_file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-07 16:04:25,268 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(hesap_Noun)(-)(hesap:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "\n",
      "['hesap']\n",
      "2023-04-07 16:04:25,270 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(i√ß_Noun)(-)(i√ß:noun_S + a3sg_S + in:p2sg_S + e:dat_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,271 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(i√ß_Noun)(-)(i√ß:noun_S + a3sg_S + i:p3sg_S + ne:dat_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,272 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(i√ß_Adj)(-)(i√ß:adjectiveRoot_ST + adjZeroDeriv_S + noun_S + a3sg_S + in:p2sg_S + e:dat_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,273 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(i√ß_Adj)(-)(i√ß:adjectiveRoot_ST + adjZeroDeriv_S + noun_S + a3sg_S + i:p3sg_S + ne:dat_ST)>\n",
      "\n",
      "['i√ß']\n",
      "2023-04-07 16:04:25,275 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(girmek_Verb)(-)(gir:verbRoot_S + me:vNeg_S + di:vPast_S + k:vA1pl_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,275 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(girmek_Verb)(-)(gir:verbRoot_S + me:vNeg_S + dik:vPastPart_S + adjAfterVerb_S + aPnon_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,276 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(girmek_Verb)(-)(gir:verbRoot_S + me:vNeg_S + dik:vPastPart_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "\n",
      "['girmek']\n",
      "2023-04-07 16:04:25,279 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(se√ßim_Noun)(-)(se√ßim:noun_S + ler:a3pl_S + pnon_S + de:loc_ST)>\n",
      "\n",
      "['se√ßim']\n",
      "2023-04-07 16:04:25,281 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(bizlemek_Verb)(-)(bizle:verbRoot_S + r:vAor_S + vA3sg_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,282 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(bizlemek_Verb)(-)(bizle:verbRoot_S + r:vAorPart_S + adjAfterVerb_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,283 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(biz_Noun)(-)(biz:noun_S + ler:a3pl_S + pnon_S + nom_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,284 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(biz_Pron_Pers)(-)(biz:pronPers_S + ler:pA1pl_S + pPnon_S + pNom_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,285 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(biz_Noun)(-)(biz:noun_S + a3sg_S + pnon_S + nom_ST + nounZeroDeriv_S + nVerb_S + nPresent_S + ler:nA3pl_ST)>\n",
      "\n",
      "['bizlemek', 'biz']\n",
      "2023-04-07 16:04:25,287 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(birinci_Num_Ord)(-)(birinci:numeralRoot_ST)>\n",
      "\n",
      "['birinci']\n",
      "2023-04-07 16:04:25,293 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(√∂ncelik_Noun)(-)(√∂ncelik:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,296 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(√∂n_Adj)(-)(√∂n:adjectiveRoot_ST + ce:aAsIf_S + adjectiveRoot_ST + lik:ness_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,297 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(√∂nce_Noun_Time)(-)(√∂nce:noun_S + a3sg_S + pnon_S + nom_ST + lik:ness_S + noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "\n",
      "['√∂nce', '√∂n', '√∂ncelik']\n",
      "2023-04-07 16:04:25,300 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(hedef_Noun)(-)(hedef:noun_S + a3sg_S + imiz:p1pl_S + nom_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,301 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(Hedef_Noun_Prop)(-)(hedef:nounProper_S + a3sg_S + imiz:p1pl_S + nom_ST)>\n",
      "\n",
      "['Hedef', 'hedef']\n",
      "2023-04-07 16:04:25,303 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(sayƒ±n_Adj)(-)(sayƒ±n:adjectiveRoot_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,304 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(saymak_Verb)(-)(say:verbRoot_S + vImp_S + ƒ±n:vA2pl_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,306 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(say_Noun)(-)(say:noun_S + a3sg_S + pnon_S + ƒ±n:gen_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,307 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(say_Noun)(-)(say:noun_S + a3sg_S + ƒ±n:p2sg_S + nom_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,308 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(sayƒ±_Noun)(-)(sayƒ±:noun_S + a3sg_S + n:p2sg_S + nom_ST)>\n",
      "\n",
      "['sayƒ±n', 'say', 'saymak', 'sayƒ±']\n",
      "2023-04-07 16:04:25,310 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(cumhurba≈ükanƒ±_Noun)(-)(cumhurba≈ükanƒ±:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,311 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(cumhurba≈ükanƒ±_Noun)(-)(cumhurba≈ükanƒ±:noun_S + a3sg_S + p3sg_S + nom_ST)>\n",
      "\n",
      "['cumhurba≈ükanƒ±']\n",
      "2023-04-07 16:04:25,314 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(Erdoƒüan_Noun_Prop)(-)(erdoƒüan:nounProper_S + a3sg_S + pnon_S + ƒ±n:gen_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,314 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(Erdoƒüan_Noun_Prop)(-)(erdoƒüan:nounProper_S + a3sg_S + ƒ±n:p2sg_S + nom_ST)>\n",
      "\n",
      "['Erdoƒüan']\n",
      "2023-04-07 16:04:25,318 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(mayƒ±s_Noun)(-)(mayƒ±s:noun_S + a3sg_S + pnon_S + ta:loc_ST + ki:rel_S + adjectiveRoot_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,319 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(Mayƒ±s_Noun_Prop)(-)(mayƒ±s:nounProper_S + a3sg_S + pnon_S + ta:loc_ST + ki:rel_S + adjectiveRoot_ST)>\n",
      "\n",
      "['mayƒ±s', 'Mayƒ±s']\n",
      "2023-04-07 16:04:25,321 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(se√ßim_Noun)(-)(se√ßim:noun_S + ler:a3pl_S + pnon_S + de:loc_ST)>\n",
      "\n",
      "['se√ßim']\n",
      "2023-04-07 16:04:25,325 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(ip_Noun)(-)(ip:noun_S + a3sg_S + pnon_S + i:acc_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,325 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(ip_Noun)(-)(ip:noun_S + a3sg_S + i:p3sg_S + nom_ST)>\n",
      "\n",
      "['ip']\n",
      "2023-04-07 16:04:25,327 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(√∂n_Noun)(-)(√∂n:noun_S + a3sg_S + pnon_S + de:loc_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,328 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(√∂n_Adj)(-)(√∂n:adjectiveRoot_ST + adjZeroDeriv_S + noun_S + a3sg_S + pnon_S + de:loc_ST)>\n",
      "\n",
      "['√∂n']\n",
      "2023-04-07 16:04:25,332 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(g√∂ƒü√ºslemek_Verb)(-)(g√∂ƒü√ºsle:verbRoot_S + me:vInf2_S + noun_S + a3sg_S + si:p3sg_S + nom_ST)>\n",
      "\n",
      "['g√∂ƒü√ºslemek']\n",
      "2023-04-07 16:04:25,334 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(D√∂ne_Noun_Prop)(-)(d√∂ne:nounProper_S + a3sg_S + m:p1sg_S + nom_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,335 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(d√∂nem_Noun_Time)(-)(d√∂nem:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "\n",
      "['D√∂ne', 'd√∂nem']\n",
      "2023-04-07 16:04:25,338 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(T√ºrkiye_Noun_Prop)(-)(t√ºrkiye:nounProper_S + a3sg_S + pnon_S + yi:acc_ST)>\n",
      "\n",
      "['T√ºrkiye']\n",
      "2023-04-07 16:04:25,341 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(idare_Noun)(-)(idare:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "\n",
      "['idare']\n",
      "2023-04-07 16:04:25,343 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(etmek_Verb)(-)(et:verbRoot_S + me:vInf2_S + noun_S + a3sg_S + si:p3sg_S + nom_ST)>\n",
      "\n",
      "['etmek']\n",
      "2023-04-07 16:04:25,345 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(H√ºdapar_Noun_Prop)(-)(h√ºdapar:nounProper_S + a3sg_S + pnon_S + nom_ST)>\n",
      "\n",
      "['H√ºdapar']\n",
      "2023-04-07 16:04:25,346 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(genel_Adj)(-)(genel:adjectiveRoot_ST)>\n",
      "\n",
      "2023-04-07 16:04:25,347 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(genelmek_Verb)(-)(genel:verbRoot_S + vImp_S + vA2sg_ST)>\n",
      "\n",
      "['genel', 'genelmek']\n",
      "['ba≈ükani']\n",
      "2023-04-07 16:04:25,358 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(Zekeriya_Noun_Prop)(-)(zekeriya:nounProper_S + a3sg_S + pnon_S + nom_ST)>\n",
      "\n",
      "['Zekeriya']\n",
      "2023-04-07 16:04:25,361 - zeyrek.rulebasedanalyzer - WARNING\n",
      "Msg: APPENDING RESULT: <(Yapƒ±cƒ±oƒülu_Noun_Prop)(-)(yapƒ±cƒ±oƒülu:nounProper_S + a3sg_S + pnon_S + nom_ST)>\n",
      "\n",
      "['Yapƒ±cƒ±oƒülu']\n"
     ]
    }
   ],
   "source": [
    "import zeyrek\n",
    "\n",
    "analyzer = zeyrek.MorphAnalyzer()\n",
    "cumle = 'hesap i√ßine girmedik se√ßimlerde bizler birinci √∂ncelik hedefimiz sayƒ±n cumhurba≈ükanƒ± erdoƒüanƒ±n  mayƒ±staki se√ßimlerde ipi √∂nde g√∂ƒü√ºslemesi yeni d√∂nem t√ºrkiyeyi idare etmesi h√ºdapar genel ba≈ükani zekeriya yapƒ±cƒ±oƒülu'\n",
    "cumle_lemmatized = []\n",
    "for i in cumle.split():\n",
    "    x=print(analyzer.lemmatize(f'{i}')[0][1])\n",
    "    cumle_lemmatized.append(x)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "df['text_lemmatized'] = df['text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bu', 'tweet', '√∂rneƒüidir', '#twitter']\n",
      "['Ba≈üka', 'bir', '√∂rnek', 'tweet', '#veri']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Veri k√ºmeniz\n",
    "tweets = [\"Bu tweet √∂rneƒüidir #twitter\", \"Ba≈üka bir √∂rnek tweet #veri\"]\n",
    "\n",
    "# TweetTokenizer nesnesini olu≈üturun\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "# Her bir tweet i√ßin tokenle≈ütirme i≈ülemi yapƒ±n\n",
    "for tweet in tweets:\n",
    "    tokens = tknzr.tokenize(tweet)\n",
    "    print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e452b117becdb853c2e41f888d6f0867f3b1e54387492c804eadff14cfaece26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
